# Flash Attention Wheels for Windows 

Pre-compiled Flash Attention wheels for Windows - **no build tools required**! ğŸš€

Flash Attention provides up to 10x faster attention computation for transformers. These wheels are self-compiled and ready to use.

## ğŸ¯ Available Wheels

| Flash Attention | Python | PyTorch | CUDA |
|----------------|--------|---------|------|
| 2.8.3 | 3.12 | 2.6.0 | 12.6 |
| 2.8.3 | 3.12 | 2.7.1 | 12.6 |
| 2.8.3 | 3.11 | 2.7.1 | 12.6 |
| 2.8.2 | 3.12 | 2.7.0 | 12.8 |
| 2.8.3 | 3.12 | 2.7.1 | 12.8 |
| 2.8.0.post2 | 3.10 | 2.5.0 | 12.4 |
| 2.7.4.post1 | 3.11 | 2.7.4 | 12.4 |



## ğŸ¤ Contributing

Found these wheels helpful? Star the repo! â­

Need a different configuration? Open an issue!

---
